{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Toxicity Classification Technical Report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Malia and Tommy chose to do research on a mushroom dataset. Malia and Tommy used classifiers KNN, and Naive Bayes, random forrest You will see below that ______ performed the best.\n",
    "\n",
    "\n",
    "\n",
    "with an accuracy of _______ and recall of ____________, correctly classifiying _________ instances of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mysklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmysklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmypytable\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmypytable\u001b[39;00m\n\u001b[1;32m      4\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(mypytable)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmysklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmypytable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MyPyTable\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mysklearn'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import mysklearn.mypytable as mypytable\n",
    "importlib.reload(mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "import mysklearn.myknnclassifier\n",
    "importlib.reload(mysklearn.myknnclassifier)\n",
    "from mysklearn.myknnclassifier import MyKNeighborsClassifier\n",
    "\n",
    "import mysklearn.mynaivebayesclassifier\n",
    "importlib.reload(mysklearn.mynaivebayesclassifier)\n",
    "from mysklearn.mynaivebayesclassifier import MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myeval\n",
    "importlib.reload(mysklearn.myeval)\n",
    "import mysklearn.myeval as myeval\n",
    "\n",
    "\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "import utils as utils\n",
    "\n",
    "mush_data = MyPyTable()\n",
    "mush_data.load_from_file('/home/CPSC322finalProject/input_data/new_mushroom_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 DATA ANALYSIS\n",
    "\n",
    "The research that Malia and Tommy did was classifiying mushrooms as either edible or non-edible. The orginal dataset was 50000 and some noise instances and they pruned it down to approxomatly 1000 of each edible and non-edible. The dataset had attributtes: cap-diameter,cap-shape,gill-attachment,gill-color,stem-height,stem-width,stem-color,season, and finaly class. The starting data was farily clean to begin with, but they had to give numeric values to seasons of the year. When conducting their research the pair used season, cap-shape, gill-attachment to classify their mushrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relevant summary stats goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.freq_plot(mush_data.data, mush_data.column_names, \"season\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caption for graph #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.freq_plot(mush_data.data, mush_data.column_names, \"cap-shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caption for graph #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.freq_plot(mush_data.data, mush_data.column_names, \"gill-attachment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caption for graph #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.freq_plot(mush_data.data, mush_data.column_names, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caption for graph #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 CLASSIFICATON RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple algorithm that classifies data points based on the class of their closest neighbors. It compares a new data point to its \"k\" nearest points in the training set and assigns the most common class among them. It relies on distance metrics like Euclidean distance.\n",
    "\n",
    "$$ d(p,q) = \\sqrt{\\sum_{i=1}^{n}(q_i - p_i)^2} $$\n",
    "\n",
    "kNN also requires normalization of data. Normalizing data involves scaling features to a standard range, often [0, 1], or a distribution with a mean of 0 and standard deviation of 1. This ensures that all features contribute equally to the model and prevents algorithms sensitive to feature scales, like k-nearest neighbors, from being biased toward larger-scale features. It improves the stability and performance of many machine learning models.\n",
    "\n",
    "$$ Xnormalized = \\frac{(x - xmin)}{(xmax - xmin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split and data set shrink\n",
    "indexes_to_remove = utils.rand_inds(50000, 54034)\n",
    "mush_data.drop_rows(indexes_to_remove)\n",
    "\n",
    "\n",
    "#print(len(mush_data.data))\n",
    "\n",
    "df_mush = []\n",
    "for i, row in enumerate(mush_data.data):\n",
    "    df_mush.append(row)\n",
    "\n",
    "#print(len(df_mush))\n",
    "\n",
    "\n",
    "season = [row[7] for row in df_mush]\n",
    "stem_width = [row[5] for row in df_mush]\n",
    "gill_color = [row[3] for row in df_mush]\n",
    "cap_diameter = [row[0] for row in df_mush]\n",
    "\n",
    "yummy_or_nah = [row[8] for row in df_mush]\n",
    "\n",
    "# normalize the data\n",
    "season_norm = utils.normalize_data(season)\n",
    "stem_width_norm = utils.normalize_data(stem_width)\n",
    "gill_color_norm = utils.normalize_data(gill_color)\n",
    "cap_diameter_norm = utils.normalize_data(cap_diameter)\n",
    "\n",
    "data_zipped = list(zip(season_norm, stem_width_norm, gill_color_norm, cap_diameter_norm))\n",
    "X_data = data_zipped\n",
    "\n",
    "X_train, X_test, y_train, y_test = myeval.train_test_split(X_data, yummy_or_nah, test_size=0.33, random_state=1, shuffle=True)\n",
    "print(len(y_test))\n",
    "print(len(X_test))\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the knn\n",
    "knn = MyKNeighborsClassifier(n_neighbors= 5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = utils.compute_accuracy(knn_y_pred, y_test)\n",
    "print(f'Accuacy: {acc:.4f}')\n",
    "\n",
    "recall = utils.compute_recall(knn_y_pred, y_test, positive_label=1)\n",
    "print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [row[-1] for row in mush_data.data]   # Labels: survived\n",
    "k = 10\n",
    "# Stratified k-fold split\n",
    "folds = myeval.stratified_kfold_split(X_data, y, n_splits=k, shuffle=True, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Naive Bayes classifier\n",
    "nb_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "# List to store the results for each fold\n",
    "nb_results = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_indices, test_indices in folds:\n",
    "    # Split the data into training and testing sets based on the current fold\n",
    "    X_train = [X_data[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X_data[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    # Train the Naive Bayes classifier\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Store the results (true labels and predicted labels)\n",
    "    nb_results.append((y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and Error Rate\n",
    "nb_accuracy, nb_error_rate = utils.calculate_accuracy_error_rate(nb_results)\n",
    "# Display the results\n",
    "print(f\"Accuracy:     {nb_accuracy:.4f}\")\n",
    "print(f\"Error Rate:   {nb_error_rate:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
